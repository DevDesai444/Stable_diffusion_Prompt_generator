{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoProcessor\n",
    "\n",
    "clip_processor = AutoProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "BATCHSIZE=128\n",
    "SAVE_OPT_CKP = True\n",
    "SAVE_MODEL_CKP = True\n",
    "UNFREEZE_START = 18 # set it to lower number when significantly more samples are included.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "\n",
    "run_name = f'clip224-l18'\n",
    "\n",
    "\n",
    "def cosine_similarity_loss(pred, target):\n",
    "    cos = nn.CosineSimilarity(dim=1)\n",
    "    output = -cos(pred, target).mean()\n",
    "    return output\n",
    "\n",
    "\n",
    "def get_train_test_split():\n",
    "    data = pd.read_csv('prompts.csv')\n",
    "    x = data.drop('prompt',axis=1)\n",
    "    y = data['prompt']\n",
    "    train_images, train_labels, test_images, test_labels = train_test_split(x, y, test_size=(0.3),random_state= 100)\n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        clip = AutoModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "        self.vision = clip.vision_model\n",
    "        self.fc = nn.Linear(1024, 384)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.vision(x)['pooler_output']\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "def load_pretrained_model():\n",
    "    model = Net()\n",
    "\n",
    "    trainable_model_weights = False\n",
    "    for name, child in model.named_children():\n",
    "        if name == 'vision':\n",
    "            for pn, p in child.named_parameters():\n",
    "                if str(UNFREEZE_START) in pn:\n",
    "                    \"\"\"start unfreezing layer , the weights are trainable\"\"\"\n",
    "                    trainable_model_weights = True\n",
    "                p.requires_grad = trainable_model_weights\n",
    "                if p.requires_grad:\n",
    "                    print(f\"{pn} is set to be trainable.\")\n",
    "\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "class IMGDataset:\n",
    "    def __init__(self, image_paths, targets, clip_processor=clip_processor):\n",
    "        self.images = image_paths\n",
    "        self.labels = targets\n",
    "        self.input_processor = clip_processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        image = Image.open(self.images[item])\n",
    "        image = self.input_processor(image)\n",
    "        target = self.labels[item]\n",
    "        return image, target\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"main training\"\"\"\n",
    "    Path(f\"../{run_name}\").mkdir(exist_ok=True)\n",
    "\n",
    "    NEPOCH=25\n",
    "    BestEpoch=0\n",
    "    BestSim = 0\n",
    "    train_images, train_targets, test_images, test_targets = get_train_test_split()\n",
    "\n",
    "    print(f\"test size: {len(test_images)}, train size: {len(train_images)}\")\n",
    "\n",
    "    nn_model = load_pretrained_model()\n",
    "    nn_model = torch.compile(nn_model)\n",
    "    optimizer = optim.AdamW(filter(lambda p: p.requires_grad, nn_model.parameters()), lr=1e-4, fused=True)\n",
    "    optimizer.zero_grad()\n",
    "    test_dataloader = DataLoader(dataset=IMGDataset(test_images, test_targets),\n",
    "                                 batch_size=BATCHSIZE, shuffle=False, num_workers=4)\n",
    "    train_dataloader = DataLoader(dataset=IMGDataset(train_images, train_targets),\n",
    "                                 batch_size=BATCHSIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "    for epoch in range(NEPOCH):\n",
    "        epoch_loss = 0\n",
    "        for s, batch_data in enumerate(tqdm(train_dataloader)):\n",
    "            batch_images, batch_targets = batch_data\n",
    "            batch_images, batch_targets = batch_images.to(device), batch_targets.to(device)\n",
    "            pred = nn_model(batch_images)\n",
    "            cosine_loss = cosine_similarity_loss(pred, batch_targets)\n",
    "            loss = cosine_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            epoch_loss += -cosine_loss.item()\n",
    "        epoch_loss /= len(train_dataloader)\n",
    "        print(f\"epoch: {epoch}, training loss: {epoch_loss}\")\n",
    "        \n",
    "        \"\"\"test loss\"\"\"\n",
    "        epoch_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_images, batch_targets in tqdm(test_dataloader):\n",
    "                batch_images, batch_targets = batch_images.to(device), batch_targets.to(device)\n",
    "                pred = nn_model(batch_images)\n",
    "                loss = -cosine_similarity_loss(pred, batch_targets)\n",
    "                epoch_loss += loss.item()\n",
    "            epoch_loss /= len(test_dataloader)\n",
    "        print(f\"epoch: {epoch}, test loss: {epoch_loss}\")\n",
    "\n",
    "        if epoch_loss > BestSim:\n",
    "            BestSim = epoch_loss\n",
    "            BestEpoch = epoch + 1\n",
    "            print(f\"save best model at {BestSim} with epoch {BestEpoch}\")\n",
    "            if SAVE_MODEL_CKP:\n",
    "                torch.save(nn_model.state_dict(), f\"{run_name}.pt\")\n",
    "            if SAVE_OPT_CKP:\n",
    "                torch.save(optimizer.state_dict(), f\"{run_name}_opt.pt\")\n",
    "\n",
    "        if epoch - 3 > BestEpoch:\n",
    "            print(f\"early stop at {epoch+1} with best epoch {BestEpoch} and test similarity {BestSim}.\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "69d80441b7a1ef29758e486985b1251723ad399c6635fd35f4640be07a6791c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
